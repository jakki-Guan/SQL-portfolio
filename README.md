# SQL Portfolio â€“ Yelp Dataset ETL & Analytics

This project demonstrates how to design and implement a **data pipeline** using **PostgreSQL, Python, and Docker**.  
I built a relational schema, processed raw JSON data from the [Yelp Open Dataset](https://www.yelp.com/dataset), and solved common real-world data challenges to make the dataset analytics-ready.

---

## ğŸš€ Key Highlights
- **ETL Pipeline**: Converted messy JSONL into clean CSV, loaded into PostgreSQL with staging â†’ final schema strategy.  
- **Schema Design**: Six core tables (`business`, `user`, `review`, `tip`, `checkin_raw`, `checkin_event`) with enforced foreign keys and data integrity.  
- **Data Cleaning & Validation**: Used Python + SQL to handle nested JSON, inconsistent date formats, orphaned rows, and duplicates.  
- **Dockerized Environment**: PostgreSQL + pgAdmin setup for reproducible local development.  
- **Analytics-Ready**: Final schema supports exploration, dashboards, and future ML pipelines.  

---

## ğŸ› ï¸ Tech Stack
- **SQL / PostgreSQL** (COPY, staging tables, ON CONFLICT, joins, arrays)  
- **Python** (pandas, JSON preprocessing, ETL scripts)  
- **Docker & Docker Compose** (database + pgAdmin setup)  

---

## ğŸ“Š Data Loading Challenges & Solutions

During ETL, I solved several issues to ensure clean and reliable data:

- **Business Table** â†’ Nested JSON (`attributes`, `hours`) â†’ converted to JSON strings via Python preprocessing.  
- **User Table** â†’ Friends/elite arrays in string format â†’ transformed with `string_to_array()` in staging â†’ final load.  
- **Review Table** â†’ Mixed date formats + orphaned rows â†’ cleaned with Python and validated with INNER JOINs.  
- **Tip & Checkin Tables** â†’ Duplicates + orphaned rows â†’ staging + `ON CONFLICT DO NOTHING` to ensure safe loads.  

ğŸ‘‰ See full details in [Data Loading Documentation](./docs/data_loading.md)

---

## ğŸ“‚ Repo Structure
## ğŸ“‚ Repo Structure

SQL-portfolio/  
â”œâ”€â”€ README.md                  # main project overview  
â”œâ”€â”€ .gitignore                 # keep raw data local  
â”œâ”€â”€ data/                      # raw & cleaned Yelp dataset (local only, ignored in Git)  
â”‚   â”œâ”€â”€ business.csv  
â”‚   â”œâ”€â”€ business_clean.csv  
â”‚   â”œâ”€â”€ checkin.csv  
â”‚   â”œâ”€â”€ review.csv  
â”‚   â”œâ”€â”€ review_clean.csv  
â”‚   â”œâ”€â”€ tip.csv  
â”‚   â”œâ”€â”€ user.csv  
â”‚   â”œâ”€â”€ yelp_academic_dataset_business.json  
â”‚   â”œâ”€â”€ yelp_academic_dataset_checkin.json  
â”‚   â”œâ”€â”€ yelp_academic_dataset_review.json  
â”‚   â”œâ”€â”€ yelp_academic_dataset_tip.json  
â”‚   â”œâ”€â”€ yelp_academic_dataset_user.json  
â”‚   â””â”€â”€ Yelp-JSON/ ...  
â”‚
â”œâ”€â”€ docker/                    # Docker setup  
â”‚   â”œâ”€â”€ docker-compose.yml  
â”‚   â”œâ”€â”€ init/  
â”‚   â””â”€â”€ sql/  
â”‚
â”œâ”€â”€ docs/                      # documentation  
â”‚   â””â”€â”€ data_loading.md  
â”‚
â”œâ”€â”€ scripts/                   # Python data cleaning scripts  
â”‚   â”œâ”€â”€ businessdatafix.py  
â”‚   â”œâ”€â”€ to_csv.py  
â”‚   â””â”€â”€ ValidateReview.py  
â”‚
â””â”€â”€ sql/                       # schema & ETL SQL scripts  
    â”œâ”€â”€ 01_create_tables.sql  
    â””â”€â”€ 02_load_data.sql  


---

## ğŸŒŸ What I Learned
This project demonstrates my ability to:
- Build **robust ETL pipelines** from raw JSON to a normalized relational schema.  
- Ensure **data integrity** with validation, staging/final schema strategy, and safe load patterns.  
- Combine **Python + SQL** for preprocessing, cleaning, and transformation.  
- Prepare **analytics-ready datasets** for BI dashboards or ML pipelines.  

---
